{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 데이터 전처리 child_num 삭제x / split 0.3",
   "metadata": {
    "cell_id": "41f480fb-6788-4a90-a44d-6346ca92cf55",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 81.953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00001-52073a70-15dd-429d-811c-e99ce5585253",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1037.96875
   },
   "source": "# 윤민님 직업 예측 수정 train4.csv\n# 전처리 진행\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd   \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import datasets, model_selection\n\ntrain= pd.read_csv('train4.csv')\n\n#train4.csv 결측치행, index 열 제거\ntrain.dropna(inplace=True)\ntrain.drop(['Unnamed: 0'], axis=1, inplace=True)\n\n# 성별 분류\ntrain['gender'] = train['gender'].replace(['F','M'],[0,1])\n\n# 차량 소유 분류\ntrain['car'] = train['car'].replace(['N','Y'],[0,1])\n\n# 부동산(주택) 소유 분류\ntrain['reality'] = train['reality'].replace(['N','Y'],[0,1])\ntrain['family_type'] = train['family_type'].replace(['Married','Civil marriage','Single / not married','Separated','Widow'],[1,1,0,0,0])\n\ntrain.loc[train['family_size'] >= 5,'family_size']=5\ntrain.loc[train['child_num'] >= 3,'child_num']=3\n\ntrain['age'] = train['DAYS_BIRTH']*(-1)/365\ntrain['age'] = train['age'].round(0)\n\n# train['begin_month'] = train['begin_month']* -1\n# train.loc[train['DAYS_EMPLOYED'] >0,'DAYS_EMPLOYED']=0\n# train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED']* -1\ntrain.drop('DAYS_BIRTH',axis=1,inplace=True)\n\ntrain['age']= -train['age']\n# test['age']= -test['age']\ntrain['income_total'] = train['income_total']/10000 \n\n\ntrain_target = train[['credit']].copy()\ntrain_data = train.drop('credit',axis=1).copy()\n\n\nx_train, x_test, y_train, y_test = model_selection.train_test_split(train_data, \n                                                                    train_target, \n                                                                    test_size=0.3, \n                                                                    random_state=0)\n",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 표준화 , 원-핫 인코딩",
   "metadata": {
    "cell_id": "00002-3e8e2ee8-1d01-4bf5-b53b-c3bee87b15cc",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 81.953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00003-343e98bf-9e3e-4835-93f8-657919799b34",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 227.96875
   },
   "source": "numeric_features = ['child_num','income_total','DAYS_EMPLOYED','family_size', 'begin_month', 'age'] \nnumeric_transformer = StandardScaler() # cf) RobustScaler\n\ncategorical_features = ['gender', 'car', 'reality','income_type','edu_type', 'family_type', 'house_type','work_phone','phone', 'email', 'occyp_type']\ncategorical_transformer = OneHotEncoder(categories='auto', handle_unknown='ignore') # categories='auto' : just for ignoring warning messages\n\npreprocessor = ColumnTransformer(\n    transformers=[ # List of (name, transformer, column(s))\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-1e0ca799-39b6-4616-b837-52cb3c1b773c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 65.953125
   },
   "source": "preprocessor_pipe = Pipeline(steps=[('preprocessor', preprocessor)]) # preprocessing-only",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00005-dc39bf1e-3784-40a9-be93-75aeff3e1d60",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 420.59375,
    "deepnote_output_heights": [
     251.640625
    ]
   },
   "source": "x_train_transformed = preprocessor_pipe.fit_transform(x_train)\nx_test_transformed = preprocessor_pipe.transform(x_test)\n\n# ex) preprocessor_pipe.transform(x_train).todense()\nx_train_transformed",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.79873732,  0.25114229,  0.05136773, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.59963645,  0.20577259, -0.92394361, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.59963645, -0.74699111, -0.92394361, ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [-0.59963645,  0.84094839,  2.470391  , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 3.59548487, -0.52014261, -0.64851234, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.59963645, -0.06644561,  0.92621781, ...,  0.        ,\n         0.        ,  0.        ]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-6dc3d5fe-d98c-4828-ace7-f7fc1032c181",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 479.03125,
    "deepnote_output_heights": [
     382.078125
    ]
   },
   "source": "x_train.isna()",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>car</th>\n      <th>reality</th>\n      <th>child_num</th>\n      <th>income_total</th>\n      <th>income_type</th>\n      <th>edu_type</th>\n      <th>family_type</th>\n      <th>house_type</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>work_phone</th>\n      <th>phone</th>\n      <th>email</th>\n      <th>occyp_type</th>\n      <th>family_size</th>\n      <th>begin_month</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2703</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>23360</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3586</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15917</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8462</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13123</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>19648</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9845</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>10799</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2732</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>17376 rows × 17 columns</p>\n</div>",
      "text/plain": "       gender    car  reality  child_num  income_total  income_type  edu_type  \\\n2703    False  False    False      False         False        False     False   \n23360   False  False    False      False         False        False     False   \n3586    False  False    False      False         False        False     False   \n15917   False  False    False      False         False        False     False   \n8462    False  False    False      False         False        False     False   \n...       ...    ...      ...        ...           ...          ...       ...   \n13123   False  False    False      False         False        False     False   \n19648   False  False    False      False         False        False     False   \n9845    False  False    False      False         False        False     False   \n10799   False  False    False      False         False        False     False   \n2732    False  False    False      False         False        False     False   \n\n       family_type  house_type  DAYS_EMPLOYED  work_phone  phone  email  \\\n2703         False       False          False       False  False  False   \n23360        False       False          False       False  False  False   \n3586         False       False          False       False  False  False   \n15917        False       False          False       False  False  False   \n8462         False       False          False       False  False  False   \n...            ...         ...            ...         ...    ...    ...   \n13123        False       False          False       False  False  False   \n19648        False       False          False       False  False  False   \n9845         False       False          False       False  False  False   \n10799        False       False          False       False  False  False   \n2732         False       False          False       False  False  False   \n\n       occyp_type  family_size  begin_month    age  \n2703        False        False        False  False  \n23360       False        False        False  False  \n3586        False        False        False  False  \n15917       False        False        False  False  \n8462        False        False        False  False  \n...           ...          ...          ...    ...  \n13123       False        False        False  False  \n19648       False        False        False  False  \n9845        False        False        False  False  \n10799       False        False        False  False  \n2732        False        False        False  False  \n\n[17376 rows x 17 columns]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00007-a8ffb781-a82f-4ac2-b5b1-6bda09b9a468",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 454.171875,
    "deepnote_output_heights": [
     213.234375
    ]
   },
   "source": "# 6개의 numeric variables & 11개의 categorical variables (각각 6개 & 11개 카테고리) \n# -> num 6 + cat : 48  = 54개의 새로운 열\n# occyp_type : 18\n# income_type :5\n# edu_type : 5\n# house_type : 6\n# gen,car, reality, family_type, work_phone, phone, email : 7*2\n\nx_train_transformed[0]",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.79873732,  0.25114229,  0.05136773,  0.89937497, -0.55290651,\n       -0.84465783,  1.        ,  0.        ,  1.        ,  0.        ,\n        0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n        1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        1.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 모델 학습, 평가",
   "metadata": {
    "cell_id": "00008-d01638c1-5b8c-466d-aa78-8e8ef7ba33be",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 81.953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00009-d096985b-31b2-4b9c-b069-582f23049a58",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 65.953125
   },
   "source": "pip install bayesian-optimization",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00010-07cfabd5-1008-4f17-8d04-128f1cd457b6",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 65.953125
   },
   "source": "pip install lightgbm",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# LightGBM",
   "metadata": {
    "cell_id": "00011-9a37717a-d7fb-4a7d-8215-af9fe60541f2",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 81.953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00012-2c76d465-0e80-4704-9bd9-74a24c64a15d",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 2037.90625
   },
   "source": "# feature 이름별로 그래프가 그려지게 시각화\n\n# plot_importance() 를 이용한 피처 중요도 시각화\n# 트리 기반의 모델은 트리를 만드는 결정에서 각 특성이 얼마나 중요한지 평가한 특성 중요도를 볼 수 있다.\n\n#column_2 : reality\n# 1 : car\n# 5 : income_type\n# 4 : income_total\n# LightGBM 임포트\nfrom lightgbm import LGBMClassifier\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import plot_importance\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n# import Common_Module.CMStat as CO\n\ndataset = load_breast_cancer()\nftr = dataset.data\ntarget = dataset.target\n#Best params: {'classifier__learning_rate': 0.01, 'classifier__loss': 'deviance', 'classifier__max_depth': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200, 'classifier__random_state': 0}\n\nlgbm_wrapper = LGBMClassifier(n_estimators=400)\nevals = [(x_test_transformed, y_test)]\nlgbm_wrapper.fit(x_train_transformed, y_train, early_stopping_rounds=100, eval_metric='multi_logloss', eval_set=evals, verbose=True)\npred = lgbm_wrapper.predict(x_test_transformed)\npred_proba = lgbm_wrapper.predict_proba(x_test_transformed)[:1]\n# CO.get_clf_eval(y_test, pred)\n\n# fig, ax = plt.subplots(figsize=(10,12))\n# plot_importance(lgbm_wrapper, ax=ax)\n# plt.show() #54개의 feature가 전부 출력됨",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1]\tvalid_0's multi_logloss: 0.867502\n[2]\tvalid_0's multi_logloss: 0.855081\n[3]\tvalid_0's multi_logloss: 0.845431\n[4]\tvalid_0's multi_logloss: 0.838082\n[5]\tvalid_0's multi_logloss: 0.83179\n[6]\tvalid_0's multi_logloss: 0.826421\n[7]\tvalid_0's multi_logloss: 0.822407\n[8]\tvalid_0's multi_logloss: 0.818831\n[9]\tvalid_0's multi_logloss: 0.815652\n[10]\tvalid_0's multi_logloss: 0.813254\n[11]\tvalid_0's multi_logloss: 0.810655\n[12]\tvalid_0's multi_logloss: 0.808802\n[13]\tvalid_0's multi_logloss: 0.807275\n[14]\tvalid_0's multi_logloss: 0.805667\n[15]\tvalid_0's multi_logloss: 0.804357\n[16]\tvalid_0's multi_logloss: 0.802724\n[17]\tvalid_0's multi_logloss: 0.801655\n[18]\tvalid_0's multi_logloss: 0.800714\n[19]\tvalid_0's multi_logloss: 0.799965\n[20]\tvalid_0's multi_logloss: 0.798742\n[21]\tvalid_0's multi_logloss: 0.798225\n[22]\tvalid_0's multi_logloss: 0.797452\n[23]\tvalid_0's multi_logloss: 0.796712\n[24]\tvalid_0's multi_logloss: 0.795879\n[25]\tvalid_0's multi_logloss: 0.795089\n[26]\tvalid_0's multi_logloss: 0.794432\n[27]\tvalid_0's multi_logloss: 0.793846\n[28]\tvalid_0's multi_logloss: 0.793097\n[29]\tvalid_0's multi_logloss: 0.792244\n[30]\tvalid_0's multi_logloss: 0.791689\n[31]\tvalid_0's multi_logloss: 0.791198\n[32]\tvalid_0's multi_logloss: 0.790246\n[33]\tvalid_0's multi_logloss: 0.789989\n[34]\tvalid_0's multi_logloss: 0.78914\n[35]\tvalid_0's multi_logloss: 0.788588\n[36]\tvalid_0's multi_logloss: 0.788197\n[37]\tvalid_0's multi_logloss: 0.787801\n[38]\tvalid_0's multi_logloss: 0.787456\n[39]\tvalid_0's multi_logloss: 0.78712\n[40]\tvalid_0's multi_logloss: 0.786661\n[41]\tvalid_0's multi_logloss: 0.78603\n[42]\tvalid_0's multi_logloss: 0.785613\n[43]\tvalid_0's multi_logloss: 0.7854\n[44]\tvalid_0's multi_logloss: 0.78498\n[45]\tvalid_0's multi_logloss: 0.78464\n[46]\tvalid_0's multi_logloss: 0.784454\n[47]\tvalid_0's multi_logloss: 0.784181\n[48]\tvalid_0's multi_logloss: 0.783858\n[49]\tvalid_0's multi_logloss: 0.783581\n[50]\tvalid_0's multi_logloss: 0.783276\n[51]\tvalid_0's multi_logloss: 0.783037\n[52]\tvalid_0's multi_logloss: 0.782963\n[53]\tvalid_0's multi_logloss: 0.782684\n[54]\tvalid_0's multi_logloss: 0.782491\n[55]\tvalid_0's multi_logloss: 0.781935\n[56]\tvalid_0's multi_logloss: 0.781837\n[57]\tvalid_0's multi_logloss: 0.781705\n[58]\tvalid_0's multi_logloss: 0.781703\n[59]\tvalid_0's multi_logloss: 0.781445\n[60]\tvalid_0's multi_logloss: 0.78124\n[61]\tvalid_0's multi_logloss: 0.781256\n[62]\tvalid_0's multi_logloss: 0.780892\n[63]\tvalid_0's multi_logloss: 0.780423\n[64]\tvalid_0's multi_logloss: 0.780077\n[65]\tvalid_0's multi_logloss: 0.780064\n[66]\tvalid_0's multi_logloss: 0.779964\n[67]\tvalid_0's multi_logloss: 0.779569\n[68]\tvalid_0's multi_logloss: 0.778891\n[69]\tvalid_0's multi_logloss: 0.778785\n[70]\tvalid_0's multi_logloss: 0.778562\n[71]\tvalid_0's multi_logloss: 0.77842\n[72]\tvalid_0's multi_logloss: 0.778224\n[73]\tvalid_0's multi_logloss: 0.778185\n[74]\tvalid_0's multi_logloss: 0.777931\n[75]\tvalid_0's multi_logloss: 0.777566\n[76]\tvalid_0's multi_logloss: 0.777386\n[77]\tvalid_0's multi_logloss: 0.777096\n[78]\tvalid_0's multi_logloss: 0.7768\n[79]\tvalid_0's multi_logloss: 0.776878\n[80]\tvalid_0's multi_logloss: 0.776863\n[81]\tvalid_0's multi_logloss: 0.776602\n[82]\tvalid_0's multi_logloss: 0.776299\n[83]\tvalid_0's multi_logloss: 0.776032\n[84]\tvalid_0's multi_logloss: 0.775576\n[85]\tvalid_0's multi_logloss: 0.775256\n[86]\tvalid_0's multi_logloss: 0.775066\n[87]\tvalid_0's multi_logloss: 0.774794\n[88]\tvalid_0's multi_logloss: 0.774725\n[89]\tvalid_0's multi_logloss: 0.774547\n[90]\tvalid_0's multi_logloss: 0.774489\n[91]\tvalid_0's multi_logloss: 0.774349\n[92]\tvalid_0's multi_logloss: 0.774291\n[93]\tvalid_0's multi_logloss: 0.774105\n[94]\tvalid_0's multi_logloss: 0.773656\n[95]\tvalid_0's multi_logloss: 0.773292\n[96]\tvalid_0's multi_logloss: 0.773292\n[97]\tvalid_0's multi_logloss: 0.773316\n[98]\tvalid_0's multi_logloss: 0.773088\n[99]\tvalid_0's multi_logloss: 0.772746\n[100]\tvalid_0's multi_logloss: 0.772776\n[101]\tvalid_0's multi_logloss: 0.772627\n[102]\tvalid_0's multi_logloss: 0.772707\n[103]\tvalid_0's multi_logloss: 0.772689\n[104]\tvalid_0's multi_logloss: 0.772566\n[105]\tvalid_0's multi_logloss: 0.772471\n[106]\tvalid_0's multi_logloss: 0.77207\n[107]\tvalid_0's multi_logloss: 0.772117\n[108]\tvalid_0's multi_logloss: 0.771944\n[109]\tvalid_0's multi_logloss: 0.771925\n[110]\tvalid_0's multi_logloss: 0.771982\n[111]\tvalid_0's multi_logloss: 0.771816\n[112]\tvalid_0's multi_logloss: 0.771723\n[113]\tvalid_0's multi_logloss: 0.771644\n[114]\tvalid_0's multi_logloss: 0.771653\n[115]\tvalid_0's multi_logloss: 0.771319\n[116]\tvalid_0's multi_logloss: 0.771071\n[117]\tvalid_0's multi_logloss: 0.770974\n[118]\tvalid_0's multi_logloss: 0.770946\n[119]\tvalid_0's multi_logloss: 0.770911\n[120]\tvalid_0's multi_logloss: 0.770834\n[121]\tvalid_0's multi_logloss: 0.770622\n[122]\tvalid_0's multi_logloss: 0.770605\n[123]\tvalid_0's multi_logloss: 0.770262\n[124]\tvalid_0's multi_logloss: 0.770199\n[125]\tvalid_0's multi_logloss: 0.770196\n[126]\tvalid_0's multi_logloss: 0.770257\n[127]\tvalid_0's multi_logloss: 0.770205\n[128]\tvalid_0's multi_logloss: 0.770322\n[129]\tvalid_0's multi_logloss: 0.770208\n[130]\tvalid_0's multi_logloss: 0.770123\n[131]\tvalid_0's multi_logloss: 0.770025\n[132]\tvalid_0's multi_logloss: 0.770039\n[133]\tvalid_0's multi_logloss: 0.769716\n[134]\tvalid_0's multi_logloss: 0.769788\n[135]\tvalid_0's multi_logloss: 0.769808\n[136]\tvalid_0's multi_logloss: 0.769743\n[137]\tvalid_0's multi_logloss: 0.769918\n[138]\tvalid_0's multi_logloss: 0.769826\n[139]\tvalid_0's multi_logloss: 0.769863\n[140]\tvalid_0's multi_logloss: 0.769946\n[141]\tvalid_0's multi_logloss: 0.769861\n[142]\tvalid_0's multi_logloss: 0.769786\n[143]\tvalid_0's multi_logloss: 0.769952\n[144]\tvalid_0's multi_logloss: 0.770087\n[145]\tvalid_0's multi_logloss: 0.770032\n[146]\tvalid_0's multi_logloss: 0.769809\n[147]\tvalid_0's multi_logloss: 0.76969\n[148]\tvalid_0's multi_logloss: 0.769653\n[149]\tvalid_0's multi_logloss: 0.769349\n[150]\tvalid_0's multi_logloss: 0.769334\n[151]\tvalid_0's multi_logloss: 0.769092\n[152]\tvalid_0's multi_logloss: 0.769378\n[153]\tvalid_0's multi_logloss: 0.769597\n[154]\tvalid_0's multi_logloss: 0.769472\n[155]\tvalid_0's multi_logloss: 0.769539\n[156]\tvalid_0's multi_logloss: 0.769593\n[157]\tvalid_0's multi_logloss: 0.76949\n[158]\tvalid_0's multi_logloss: 0.769456\n[159]\tvalid_0's multi_logloss: 0.769362\n[160]\tvalid_0's multi_logloss: 0.769339\n[161]\tvalid_0's multi_logloss: 0.769344\n[162]\tvalid_0's multi_logloss: 0.769182\n[163]\tvalid_0's multi_logloss: 0.769006\n[164]\tvalid_0's multi_logloss: 0.768901\n[165]\tvalid_0's multi_logloss: 0.768845\n[166]\tvalid_0's multi_logloss: 0.768821\n[167]\tvalid_0's multi_logloss: 0.768868\n[168]\tvalid_0's multi_logloss: 0.768962\n[169]\tvalid_0's multi_logloss: 0.768825\n[170]\tvalid_0's multi_logloss: 0.768789\n[171]\tvalid_0's multi_logloss: 0.768539\n[172]\tvalid_0's multi_logloss: 0.768611\n[173]\tvalid_0's multi_logloss: 0.768485\n[174]\tvalid_0's multi_logloss: 0.768525\n[175]\tvalid_0's multi_logloss: 0.768465\n[176]\tvalid_0's multi_logloss: 0.768515\n[177]\tvalid_0's multi_logloss: 0.768551\n[178]\tvalid_0's multi_logloss: 0.768478\n[179]\tvalid_0's multi_logloss: 0.768367\n[180]\tvalid_0's multi_logloss: 0.768315\n[181]\tvalid_0's multi_logloss: 0.768239\n[182]\tvalid_0's multi_logloss: 0.76841\n[183]\tvalid_0's multi_logloss: 0.768723\n[184]\tvalid_0's multi_logloss: 0.768844\n[185]\tvalid_0's multi_logloss: 0.768524\n[186]\tvalid_0's multi_logloss: 0.768385\n[187]\tvalid_0's multi_logloss: 0.768426\n[188]\tvalid_0's multi_logloss: 0.76857\n[189]\tvalid_0's multi_logloss: 0.768742\n[190]\tvalid_0's multi_logloss: 0.768634\n[191]\tvalid_0's multi_logloss: 0.768684\n[192]\tvalid_0's multi_logloss: 0.768839\n[193]\tvalid_0's multi_logloss: 0.769101\n[194]\tvalid_0's multi_logloss: 0.768967\n[195]\tvalid_0's multi_logloss: 0.768961\n[196]\tvalid_0's multi_logloss: 0.768914\n[197]\tvalid_0's multi_logloss: 0.768904\n[198]\tvalid_0's multi_logloss: 0.768918\n[199]\tvalid_0's multi_logloss: 0.769104\n[200]\tvalid_0's multi_logloss: 0.769187\n[201]\tvalid_0's multi_logloss: 0.769214\n[202]\tvalid_0's multi_logloss: 0.769034\n[203]\tvalid_0's multi_logloss: 0.768998\n[204]\tvalid_0's multi_logloss: 0.769035\n[205]\tvalid_0's multi_logloss: 0.769161\n[206]\tvalid_0's multi_logloss: 0.769179\n[207]\tvalid_0's multi_logloss: 0.769189\n[208]\tvalid_0's multi_logloss: 0.769345\n[209]\tvalid_0's multi_logloss: 0.769409\n[210]\tvalid_0's multi_logloss: 0.769464\n[211]\tvalid_0's multi_logloss: 0.769356\n[212]\tvalid_0's multi_logloss: 0.769329\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[213]\tvalid_0's multi_logloss: 0.769453\n[214]\tvalid_0's multi_logloss: 0.769354\n[215]\tvalid_0's multi_logloss: 0.769359\n[216]\tvalid_0's multi_logloss: 0.76947\n[217]\tvalid_0's multi_logloss: 0.769613\n[218]\tvalid_0's multi_logloss: 0.769776\n[219]\tvalid_0's multi_logloss: 0.769873\n[220]\tvalid_0's multi_logloss: 0.769974\n[221]\tvalid_0's multi_logloss: 0.770049\n[222]\tvalid_0's multi_logloss: 0.770095\n[223]\tvalid_0's multi_logloss: 0.770116\n[224]\tvalid_0's multi_logloss: 0.770236\n[225]\tvalid_0's multi_logloss: 0.770182\n[226]\tvalid_0's multi_logloss: 0.770363\n[227]\tvalid_0's multi_logloss: 0.770369\n[228]\tvalid_0's multi_logloss: 0.770456\n[229]\tvalid_0's multi_logloss: 0.770736\n[230]\tvalid_0's multi_logloss: 0.770723\n[231]\tvalid_0's multi_logloss: 0.770916\n[232]\tvalid_0's multi_logloss: 0.771027\n[233]\tvalid_0's multi_logloss: 0.771088\n[234]\tvalid_0's multi_logloss: 0.771428\n[235]\tvalid_0's multi_logloss: 0.771387\n[236]\tvalid_0's multi_logloss: 0.771398\n[237]\tvalid_0's multi_logloss: 0.771408\n[238]\tvalid_0's multi_logloss: 0.771484\n[239]\tvalid_0's multi_logloss: 0.771408\n[240]\tvalid_0's multi_logloss: 0.771495\n[241]\tvalid_0's multi_logloss: 0.771605\n[242]\tvalid_0's multi_logloss: 0.771738\n[243]\tvalid_0's multi_logloss: 0.771804\n[244]\tvalid_0's multi_logloss: 0.771808\n[245]\tvalid_0's multi_logloss: 0.771666\n[246]\tvalid_0's multi_logloss: 0.771753\n[247]\tvalid_0's multi_logloss: 0.771976\n[248]\tvalid_0's multi_logloss: 0.772171\n[249]\tvalid_0's multi_logloss: 0.772116\n[250]\tvalid_0's multi_logloss: 0.772311\n[251]\tvalid_0's multi_logloss: 0.772339\n[252]\tvalid_0's multi_logloss: 0.772406\n[253]\tvalid_0's multi_logloss: 0.772351\n[254]\tvalid_0's multi_logloss: 0.772433\n[255]\tvalid_0's multi_logloss: 0.772699\n[256]\tvalid_0's multi_logloss: 0.772899\n[257]\tvalid_0's multi_logloss: 0.772991\n[258]\tvalid_0's multi_logloss: 0.773048\n[259]\tvalid_0's multi_logloss: 0.773263\n[260]\tvalid_0's multi_logloss: 0.773433\n[261]\tvalid_0's multi_logloss: 0.773538\n[262]\tvalid_0's multi_logloss: 0.773673\n[263]\tvalid_0's multi_logloss: 0.773774\n[264]\tvalid_0's multi_logloss: 0.773768\n[265]\tvalid_0's multi_logloss: 0.773819\n[266]\tvalid_0's multi_logloss: 0.773848\n[267]\tvalid_0's multi_logloss: 0.773856\n[268]\tvalid_0's multi_logloss: 0.773838\n[269]\tvalid_0's multi_logloss: 0.773995\n[270]\tvalid_0's multi_logloss: 0.774164\n[271]\tvalid_0's multi_logloss: 0.774345\n[272]\tvalid_0's multi_logloss: 0.774479\n[273]\tvalid_0's multi_logloss: 0.774665\n[274]\tvalid_0's multi_logloss: 0.774754\n[275]\tvalid_0's multi_logloss: 0.774913\n[276]\tvalid_0's multi_logloss: 0.774981\n[277]\tvalid_0's multi_logloss: 0.775254\n[278]\tvalid_0's multi_logloss: 0.775308\n[279]\tvalid_0's multi_logloss: 0.775515\n[280]\tvalid_0's multi_logloss: 0.77566\n[281]\tvalid_0's multi_logloss: 0.775728\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 그리드 서치\n# 평가 지표 (logloss , macro)",
   "metadata": {
    "cell_id": "00013-5065e413-6650-4252-a391-1a9618193d0e",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 159.9375
   }
  },
  {
   "cell_type": "code",
   "source": "# 그리드 서치\nfrom sklearn.model_selection import GridSearchCV \nimport lightgbm as lgb \nlgb=lgb.LGBMClassifier()\n#Define the parameters \n\n\nparameters = {'num_leaves':[20,40,60,80,100], 'min_child_samples':[5,10,15],'max_depth':[-1,5,10,20], 'learning_rate':[0.05,0.1,0.2],'reg_alpha':[0,0.01,0.03]}#Define the scoring \nclf=GridSearchCV(lgb,parameters,scoring='accuracy') \nclf.fit(X=x_train_transformed, y=y_train) \nprint(clf.best_params_) \npredicted=clf.predict(x_test_transformed) \nprint('Classification of the result is:')\nprint(accuracy_score(y_test, predicted)) \n# {'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 15, 'num_leaves': 60, 'reg_alpha': 0.01}",
   "metadata": {
    "cell_id": "a54240168bb7484f9fb552e75ab797ab",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 317.96875
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00014-dee1035f-21bb-45cc-b821-42b187fb8b71",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1582.9375
   },
   "source": "# LightGBM 임포트\nfrom lightgbm import LGBMClassifier\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n\n\nparams = {'learning_rate' : 0.1,\n          'max_depth' : -1, #트리의 최대 깊이를 의미\n          'n_estimators' : 100, #반복 수행하는 트리의 개수를 의미한다\n          'min_child_samples' : 15, #최종 결정 클래스인 Leaf Node가 되기 위해서 최소한으로 필요한 데이터 개체의 수를 의미하며, \n                                   # 과적합을 제어하는 파라미터이다\n          'objective' : 'multiclass', #다중 분류\n          'num_class': 3, # 클래스 3개 (0, 1, 2)\n          'metric' : 'multi_logloss',\n          'boosting_type' : 'gbdt', # gbdt는 일반적인 GB 결정 트리 알고리즘 (gradient boosting decision tree)\n          'n_jobs' : -1,\n          'reg_alpha': 0.01, #L1 정규화를 위한 값\n          'num_leaves': 60, \n          }\n# LGBM 분류기 객체 생성\nlgbm_wrapper = LGBMClassifier(**params)\n\n# 학습, 테스트 데이터 분리\n# X_train, X_test, y_train, y_test = train_test_split(ftr, target, test_size=0.2)\n\n# model.fit(x_train_transformed, y_train)\n\n\n# 조기 중단 기능에 필요한 파라미터 정의\nevals = [(x_test_transformed, y_test)]\nlgbm_wrapper.fit(x_train_transformed, y_train, eval_metric='multi_logloss', eval_set=evals, verbose=True)\n\npreds = lgbm_wrapper.predict(x_test_transformed)\npred_proba = lgbm_wrapper.predict_proba(x_test_transformed)[:,1]\n\n\n# 다양한 오차 측정 지표를 확인하기 위한 함수 정의\n\nfrom sklearn.metrics import *\n\nprint(\"Precision Score 정밀도 : \",precision_score(y_test, predicted, pos_label='positive',average='macro'))\nprint(\"Recall Score 재현율 : \",recall_score(y_test, predicted, \n                                           pos_label='positive'\n                                           ,average='macro')) \nprint ('F1 정밀도와 재현율의 조화평균: ', round(f1_score(y_test, predicted, average='macro'), ndigits=4))\nprint ('Confusion 혼동행렬: \\n', confusion_matrix(y_test, predicted))",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1]\tvalid_0's multi_logloss: 0.866845\n[2]\tvalid_0's multi_logloss: 0.853139\n[3]\tvalid_0's multi_logloss: 0.843271\n[4]\tvalid_0's multi_logloss: 0.835305\n[5]\tvalid_0's multi_logloss: 0.828628\n[6]\tvalid_0's multi_logloss: 0.823379\n[7]\tvalid_0's multi_logloss: 0.818241\n[8]\tvalid_0's multi_logloss: 0.813998\n[9]\tvalid_0's multi_logloss: 0.810795\n[10]\tvalid_0's multi_logloss: 0.807598\n[11]\tvalid_0's multi_logloss: 0.804262\n[12]\tvalid_0's multi_logloss: 0.801983\n[13]\tvalid_0's multi_logloss: 0.800288\n[14]\tvalid_0's multi_logloss: 0.798227\n[15]\tvalid_0's multi_logloss: 0.796948\n[16]\tvalid_0's multi_logloss: 0.795744\n[17]\tvalid_0's multi_logloss: 0.794448\n[18]\tvalid_0's multi_logloss: 0.792959\n[19]\tvalid_0's multi_logloss: 0.791874\n[20]\tvalid_0's multi_logloss: 0.791299\n[21]\tvalid_0's multi_logloss: 0.790098\n[22]\tvalid_0's multi_logloss: 0.78952\n[23]\tvalid_0's multi_logloss: 0.788503\n[24]\tvalid_0's multi_logloss: 0.787894\n[25]\tvalid_0's multi_logloss: 0.787137\n[26]\tvalid_0's multi_logloss: 0.786117\n[27]\tvalid_0's multi_logloss: 0.785349\n[28]\tvalid_0's multi_logloss: 0.784823\n[29]\tvalid_0's multi_logloss: 0.78394\n[30]\tvalid_0's multi_logloss: 0.783437\n[31]\tvalid_0's multi_logloss: 0.782892\n[32]\tvalid_0's multi_logloss: 0.782653\n[33]\tvalid_0's multi_logloss: 0.782269\n[34]\tvalid_0's multi_logloss: 0.781788\n[35]\tvalid_0's multi_logloss: 0.781528\n[36]\tvalid_0's multi_logloss: 0.781285\n[37]\tvalid_0's multi_logloss: 0.781073\n[38]\tvalid_0's multi_logloss: 0.780417\n[39]\tvalid_0's multi_logloss: 0.780042\n[40]\tvalid_0's multi_logloss: 0.779407\n[41]\tvalid_0's multi_logloss: 0.779128\n[42]\tvalid_0's multi_logloss: 0.778762\n[43]\tvalid_0's multi_logloss: 0.7788\n[44]\tvalid_0's multi_logloss: 0.77872\n[45]\tvalid_0's multi_logloss: 0.778564\n[46]\tvalid_0's multi_logloss: 0.778168\n[47]\tvalid_0's multi_logloss: 0.778033\n[48]\tvalid_0's multi_logloss: 0.777449\n[49]\tvalid_0's multi_logloss: 0.777135\n[50]\tvalid_0's multi_logloss: 0.77711\n[51]\tvalid_0's multi_logloss: 0.776941\n[52]\tvalid_0's multi_logloss: 0.776616\n[53]\tvalid_0's multi_logloss: 0.77593\n[54]\tvalid_0's multi_logloss: 0.775477\n[55]\tvalid_0's multi_logloss: 0.774975\n[56]\tvalid_0's multi_logloss: 0.77446\n[57]\tvalid_0's multi_logloss: 0.774317\n[58]\tvalid_0's multi_logloss: 0.774014\n[59]\tvalid_0's multi_logloss: 0.77368\n[60]\tvalid_0's multi_logloss: 0.773111\n[61]\tvalid_0's multi_logloss: 0.773133\n[62]\tvalid_0's multi_logloss: 0.772925\n[63]\tvalid_0's multi_logloss: 0.772778\n[64]\tvalid_0's multi_logloss: 0.772629\n[65]\tvalid_0's multi_logloss: 0.772642\n[66]\tvalid_0's multi_logloss: 0.772809\n[67]\tvalid_0's multi_logloss: 0.772638\n[68]\tvalid_0's multi_logloss: 0.772646\n[69]\tvalid_0's multi_logloss: 0.772468\n[70]\tvalid_0's multi_logloss: 0.772243\n[71]\tvalid_0's multi_logloss: 0.772082\n[72]\tvalid_0's multi_logloss: 0.77208\n[73]\tvalid_0's multi_logloss: 0.771975\n[74]\tvalid_0's multi_logloss: 0.771935\n[75]\tvalid_0's multi_logloss: 0.771844\n[76]\tvalid_0's multi_logloss: 0.771916\n[77]\tvalid_0's multi_logloss: 0.771578\n[78]\tvalid_0's multi_logloss: 0.771399\n[79]\tvalid_0's multi_logloss: 0.771455\n[80]\tvalid_0's multi_logloss: 0.771056\n[81]\tvalid_0's multi_logloss: 0.770821\n[82]\tvalid_0's multi_logloss: 0.770746\n[83]\tvalid_0's multi_logloss: 0.770668\n[84]\tvalid_0's multi_logloss: 0.770622\n[85]\tvalid_0's multi_logloss: 0.770766\n[86]\tvalid_0's multi_logloss: 0.770648\n[87]\tvalid_0's multi_logloss: 0.770809\n[88]\tvalid_0's multi_logloss: 0.770966\n[89]\tvalid_0's multi_logloss: 0.770888\n[90]\tvalid_0's multi_logloss: 0.770903\n[91]\tvalid_0's multi_logloss: 0.770736\n[92]\tvalid_0's multi_logloss: 0.770549\n[93]\tvalid_0's multi_logloss: 0.770597\n[94]\tvalid_0's multi_logloss: 0.770562\n[95]\tvalid_0's multi_logloss: 0.770755\n[96]\tvalid_0's multi_logloss: 0.770902\n[97]\tvalid_0's multi_logloss: 0.770572\n[98]\tvalid_0's multi_logloss: 0.770666\n[99]\tvalid_0's multi_logloss: 0.770578\n[100]\tvalid_0's multi_logloss: 0.770611\nPrecision Score 정밀도 :  0.6195689474590939\nRecall Score 재현율 :  0.4377535901736784\nF1 정밀도와 재현율의 조화평균:  0.4371\nConfusion 혼동행렬: \n [[  40  131  739]\n [  12  531 1237]\n [  34  104 4619]]\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00015-636e411e-e68c-489a-99e3-f412b19e2793",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 442.34375
   },
   "source": "#Macro-precision : 평균의 평균\n# - 각 클래스의 평가 지표를 계산한 후, 모든 클래스에 대한 평균 값을 구한다.\n#Micro-precision : 개수의 평균\n# - 모든 클래스에 대한 결과를 하나의 혼동행렬로 정리한 후, 평가 지표를 계산한다.\n\n# 클래스 불균형(Imbalance) 문제가 있는 데이터셋에서는 Micro-average가 조금 더 효과적인 평가지표가 된다\n# https://junklee.tistory.com/116\nprint(\"Precision Score 정밀도 : \",precision_score(y_test, predicted, pos_label='positive',average='micro'))\nprint(\"Recall Score 재현율 : \",recall_score(y_test, predicted, \n                                           pos_label='positive'\n                                           ,average='micro')) \nprint ('F1 정밀도와 재현율의 조화평균: ', round(f1_score(y_test, predicted, average='micro'), ndigits=4))\nprint ('Confusion 혼동행렬: \\n', confusion_matrix(y_test, predicted))",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Precision Score 정밀도 :  0.6969249362159259\nRecall Score 재현율 :  0.6969249362159259\nF1 정밀도와 재현율의 조화평균:  0.6969\nConfusion 혼동행렬: \n [[  40  131  739]\n [  12  531 1237]\n [  34  104 4619]]\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00016-bc71f2ea-ca2b-42af-8414-152e84113661",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 201.125
   },
   "source": "pred_train = clf.predict(x_train_transformed)\npred_train\nfrom collections import Counter\n\ncnt = Counter(pred_train)\nprint('가장 많이 있는 원소 차례대로 3개 뽑기 :', cnt.most_common(3))",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "가장 많이 있는 원소 차례대로 3개 뽑기 : [(2.0, 14937), (1.0, 1946), (0.0, 493)]\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-aa450b38-d2b3-4c7e-a186-371c8afae33b",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 165.125
   },
   "source": "pred_test = clf.predict(x_test_transformed)\npred_test\ncnt = Counter(pred_test)\nprint('가장 많이 있는 원소 차례대로 3개 뽑기 :', cnt.most_common(3))",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "가장 많이 있는 원소 차례대로 3개 뽑기 : [(2.0, 6595), (1.0, 766), (0.0, 86)]\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00018-f0cc16cf-0258-4ee3-ac26-56208196aa1c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 202.34375
   },
   "source": "# 모델 평가\nfrom sklearn.metrics import accuracy_score\n\nprint(accuracy_score(y_train,pred_train))\nprint(accuracy_score(y_test,pred_test))",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.7588052486187845\n0.6969249362159259\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00019-96052fa0-2bd2-406d-81d6-940f1a6dd225",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1411.9375,
    "deepnote_output_heights": [
     394.984375
    ]
   },
   "source": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, pred_test)\n# print(cm)\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['font.family'] ='Malgun Gothic'\nmatplotlib.rcParams['axes.unicode_minus'] =False\n\nmpl.rcParams['font.size'] = 20\nplt.figure(figsize=(7,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n# plt.title('lightGBM')\nplt.ylabel('예측')\nplt.xlabel('실제')\nplt.xticks([0.5, 1.5, 2.5], ['0', '1', '2'])\nplt.yticks([0.5, 1.5, 2.5], ['0','1', '2'])\nplt.show()\nprint('대각선 상의 값들이 올바른 예측')\nprint()\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\ndef metrics(y, pred_y, title=None):\n    print(title)\n#     print(\"정확도(accuracy): \", accuracy_score(y, pred_y))\n#     print(\"재현율(recall): \", recall_score(y, pred_y))\n#     print(\"정밀도(precision): \", precision_score(y, pred_y))\n#     print(\"F1 score: \", f1_score(y, pred_y))\n    print(\"정밀도(Precision Score) : \",precision_score(y_test, predicted, pos_label='positive',average='macro'))\n    print(\"재현율(Recall Score) : \",recall_score(y_test, predicted, \n                                           pos_label='positive'\n                                           ,average='macro'))\n    print ('F1-score (정밀도와 재현율의 조화평균): ', round(f1_score(y_test, predicted, average='macro'), ndigits=4))\n    print ('혼동행렬: \\n', confusion_matrix(y_test, predicted))\nmetrics(y_test, pred_test, \"lightGBM\")",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 504x432 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light",
      "image/png": {
       "width": 458,
       "height": 393
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "대각선 상의 값들이 올바른 예측\n\nlightGBM\n정밀도(Precision Score) :  0.6195689474590939\n재현율(Recall Score) :  0.4377535901736784\nF1-score (정밀도와 재현율의 조화평균):  0.4371\n혼동행렬: \n [[  40  131  739]\n [  12  531 1237]\n [  34  104 4619]]\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00020-bf8a5775-bdd5-4b5e-9038-0beff473c3d0",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 515.953125
   },
   "source": "# 직접 혼동행렬 해석하기\ncm = confusion_matrix(y_test, predicted)\nTrue_Positive = cm[0,0] + cm[1,1] + cm[2,2]\nTrue_Positive #5190\n\nTrue_Negative_for_0 = cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2] #6491\nTrue_Negative_for_1 = cm[0,0]+cm[0,2]+cm[2,0]+cm[2,2] #5432\nTrue_Negative_for_2 = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1] #714\n\nFalse_Positive_for_0 = cm[1,0]+cm[2,0] #46\nFalse_Positive_for_1 = cm[0,1]+cm[2,1] #235\nFalse_Positive_for_2 = cm[0,2]+cm[1,2] #1976\nFP = False_Positive_for_0 +False_Positive_for_1 +False_Positive_for_2 #2257\nFalse_Negative_for_0 =cm[0,1]+cm[0,2] #870\nFalse_Negative_for_1 =cm[1,0]+cm[1,2] #1249\nFalse_Negative_for_2 =cm[2,0]+cm[2,1] #138\nFN = False_Negative_for_0+False_Negative_for_1+False_Negative_for_2 #2257\nacc = True_Positive/cm.sum()\n\n# cm.sum(axis=0) #[  86,  766, 6595]\n# cm.sum(axis=1) #[ 910, 1780, 4757]\n\n\nprecision = True_Positive / (True_Positive + FP) #0.6969249362159259\nrecall = True_Positive / (True_Positive+FN) #0.6969249362159259\nF1 = 2*(precision*recall)/(precision+recall) #0.6969249362159259",
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 베이시안 최적화",
   "metadata": {
    "cell_id": "00021-ac81cae5-5a62-462a-a5a6-2c16928089d6",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 81.953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00022-05be0d34-186d-404e-9c05-b4ece4ae7ca4",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 119.953125
   },
   "source": "import lightgbm as lgbm\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.model_selection import cross_validate",
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00023-6b198b88-07b2-4736-9a58-2a9bdc9e9077",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 389.953125
   },
   "source": "#목적함수 생성\ndef lgbm_cv(learning_rate, num_leaves, max_depth, min_child_weight, colsample_bytree, feature_fraction, bagging_fraction, lambda_l1, lambda_l2):\n    model = lgbm.LGBMClassifier(learning_rate=learning_rate,\n                                n_estimators = 100,\n                                #boosting = 'dart',\n                                num_leaves = int(round(num_leaves)),\n                                max_depth = int(round(max_depth)),\n                                min_child_weight = int(round(min_child_weight)),\n                                colsample_bytree = colsample_bytree,\n                                feature_fraction = max(min(feature_fraction, 1), 0),\n                                bagging_fraction = max(min(bagging_fraction, 1), 0),\n                                lambda_l1 = max(lambda_l1, 0),\n                                lambda_l2 = max(lambda_l2, 0)\n                               )\n    scoring = {'roc_auc_score': make_scorer(roc_auc_score)}\n#     x_train, x_test, y_train, y_test\n    result = cross_validate(model, x_train_transformed, y_train, cv=5, scoring=scoring)\n    auc_score = result[\"test_roc_auc_score\"].mean()\n    return auc_score",
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00024-d56c017b-dc1e-4b24-bf77-008fedd9e2fa",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 245.953125
   },
   "source": "# 입력값의 탐색 대상 구간\npbounds = {'learning_rate' : (0.0001, 0.05),\n           'num_leaves': (300, 600),\n           'max_depth': (2, 25),\n           'min_child_weight': (30, 100),\n           'colsample_bytree': (0, 0.99),\n           'feature_fraction': (0.0001, 0.99),\n           'bagging_fraction': (0.0001, 0.99),\n           'lambda_l1' : (0, 0.99),\n           'lambda_l2' : (0, 0.99),\n          }",
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00025-46c5c102-adbe-437c-a053-82029dab90e3",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 83.953125
   },
   "source": "#객체 생성\nlgbmBO = BayesianOptimization(f = lgbm_cv, pbounds = pbounds, verbose = 2, random_state = 0 )",
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00026-0012b5cf-1a98-4102-be15-8e5d004bb432",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1468.078125
   },
   "source": "# 반복적으로 베이지안 최적화 수행\n# acq='ei'사용\n# xi=0.01 로 exploration의 강도를 조금 높임\nlgbmBO.maximize(init_points=5, n_iter = 20, acq='ei', xi=0.01)",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "|   iter    |  target   | baggin... | colsam... | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_ch... | num_le... |\n-------------------------------------------------------------------------------------------------------------------------------------\n[LightGBM] [Warning] feature_fraction is set=0.5967754659733202, colsample_bytree=0.7080374727086953 will be ignored. Current value: feature_fraction=0.5967754659733202\n[LightGBM] [Warning] lambda_l1 is set=0.5394343511669278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5394343511669278\n[LightGBM] [Warning] bagging_fraction is set=0.5433704875376587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5433704875376587\n[LightGBM] [Warning] lambda_l2 is set=0.41941825134551564, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41941825134551564\n[LightGBM] [Warning] feature_fraction is set=0.5967754659733202, colsample_bytree=0.7080374727086953 will be ignored. Current value: feature_fraction=0.5967754659733202\n[LightGBM] [Warning] lambda_l1 is set=0.5394343511669278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5394343511669278\n[LightGBM] [Warning] bagging_fraction is set=0.5433704875376587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5433704875376587\n[LightGBM] [Warning] lambda_l2 is set=0.41941825134551564, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41941825134551564\n[LightGBM] [Warning] feature_fraction is set=0.5967754659733202, colsample_bytree=0.7080374727086953 will be ignored. Current value: feature_fraction=0.5967754659733202\n[LightGBM] [Warning] lambda_l1 is set=0.5394343511669278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5394343511669278\n[LightGBM] [Warning] bagging_fraction is set=0.5433704875376587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5433704875376587\n[LightGBM] [Warning] lambda_l2 is set=0.41941825134551564, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41941825134551564\n[LightGBM] [Warning] feature_fraction is set=0.5967754659733202, colsample_bytree=0.7080374727086953 will be ignored. Current value: feature_fraction=0.5967754659733202\n[LightGBM] [Warning] lambda_l1 is set=0.5394343511669278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5394343511669278\n[LightGBM] [Warning] bagging_fraction is set=0.5433704875376587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5433704875376587\n[LightGBM] [Warning] lambda_l2 is set=0.41941825134551564, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41941825134551564\n[LightGBM] [Warning] feature_fraction is set=0.5967754659733202, colsample_bytree=0.7080374727086953 will be ignored. Current value: feature_fraction=0.5967754659733202\n[LightGBM] [Warning] lambda_l1 is set=0.5394343511669278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5394343511669278\n[LightGBM] [Warning] bagging_fraction is set=0.5433704875376587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5433704875376587\n[LightGBM] [Warning] lambda_l2 is set=0.41941825134551564, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41941825134551564\n| \u001b[0m 1       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.5434  \u001b[0m | \u001b[0m 0.708   \u001b[0m | \u001b[0m 0.5968  \u001b[0m | \u001b[0m 0.5394  \u001b[0m | \u001b[0m 0.4194  \u001b[0m | \u001b[0m 0.03233 \u001b[0m | \u001b[0m 12.06   \u001b[0m | \u001b[0m 92.42   \u001b[0m | \u001b[0m 589.1   \u001b[0m |\n[LightGBM] [Warning] feature_fraction is set=0.5236530810634001, colsample_bytree=0.7838077877018379 will be ignored. Current value: feature_fraction=0.5236530810634001\n[LightGBM] [Warning] lambda_l1 is set=0.562364115482993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.562364115482993\n[LightGBM] [Warning] bagging_fraction is set=0.37966875948563733, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37966875948563733\n[LightGBM] [Warning] lambda_l2 is set=0.9163406719097344, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9163406719097344\n[LightGBM] [Warning] feature_fraction is set=0.5236530810634001, colsample_bytree=0.7838077877018379 will be ignored. Current value: feature_fraction=0.5236530810634001\n[LightGBM] [Warning] lambda_l1 is set=0.562364115482993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.562364115482993\n[LightGBM] [Warning] bagging_fraction is set=0.37966875948563733, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37966875948563733\n[LightGBM] [Warning] lambda_l2 is set=0.9163406719097344, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9163406719097344\n[LightGBM] [Warning] feature_fraction is set=0.5236530810634001, colsample_bytree=0.7838077877018379 will be ignored. Current value: feature_fraction=0.5236530810634001\n[LightGBM] [Warning] lambda_l1 is set=0.562364115482993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.562364115482993\n[LightGBM] [Warning] bagging_fraction is set=0.37966875948563733, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37966875948563733\n[LightGBM] [Warning] lambda_l2 is set=0.9163406719097344, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9163406719097344\n[LightGBM] [Warning] feature_fraction is set=0.5236530810634001, colsample_bytree=0.7838077877018379 will be ignored. Current value: feature_fraction=0.5236530810634001\n[LightGBM] [Warning] lambda_l1 is set=0.562364115482993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.562364115482993\n[LightGBM] [Warning] bagging_fraction is set=0.37966875948563733, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37966875948563733\n[LightGBM] [Warning] lambda_l2 is set=0.9163406719097344, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9163406719097344\n[LightGBM] [Warning] feature_fraction is set=0.5236530810634001, colsample_bytree=0.7838077877018379 will be ignored. Current value: feature_fraction=0.5236530810634001\n[LightGBM] [Warning] lambda_l1 is set=0.562364115482993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.562364115482993\n[LightGBM] [Warning] bagging_fraction is set=0.37966875948563733, subsample=1.0 will be ignored. Current value: bagging_fraction=0.37966875948563733\n[LightGBM] [Warning] lambda_l2 is set=0.9163406719097344, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9163406719097344\n| \u001b[0m 2       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.3797  \u001b[0m | \u001b[0m 0.7838  \u001b[0m | \u001b[0m 0.5237  \u001b[0m | \u001b[0m 0.5624  \u001b[0m | \u001b[0m 0.9163  \u001b[0m | \u001b[0m 0.003645\u001b[0m | \u001b[0m 4.004   \u001b[0m | \u001b[0m 31.42   \u001b[0m | \u001b[0m 549.8   \u001b[0m |\n[LightGBM] [Warning] feature_fraction is set=0.968834296976213, colsample_bytree=0.8613120267643509 will be ignored. Current value: feature_fraction=0.968834296976213\n[LightGBM] [Warning] lambda_l1 is set=0.7911669785745563, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7911669785745563\n[LightGBM] [Warning] bagging_fraction is set=0.770397367765257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770397367765257\n[LightGBM] [Warning] lambda_l2 is set=0.45686456863040253, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45686456863040253\n[LightGBM] [Warning] feature_fraction is set=0.968834296976213, colsample_bytree=0.8613120267643509 will be ignored. Current value: feature_fraction=0.968834296976213\n[LightGBM] [Warning] lambda_l1 is set=0.7911669785745563, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7911669785745563\n[LightGBM] [Warning] bagging_fraction is set=0.770397367765257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770397367765257\n[LightGBM] [Warning] lambda_l2 is set=0.45686456863040253, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45686456863040253\n[LightGBM] [Warning] feature_fraction is set=0.968834296976213, colsample_bytree=0.8613120267643509 will be ignored. Current value: feature_fraction=0.968834296976213\n[LightGBM] [Warning] lambda_l1 is set=0.7911669785745563, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7911669785745563\n[LightGBM] [Warning] bagging_fraction is set=0.770397367765257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770397367765257\n[LightGBM] [Warning] lambda_l2 is set=0.45686456863040253, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45686456863040253\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[LightGBM] [Warning] feature_fraction is set=0.968834296976213, colsample_bytree=0.8613120267643509 will be ignored. Current value: feature_fraction=0.968834296976213\n[LightGBM] [Warning] lambda_l1 is set=0.7911669785745563, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7911669785745563\n[LightGBM] [Warning] bagging_fraction is set=0.770397367765257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770397367765257\n[LightGBM] [Warning] lambda_l2 is set=0.45686456863040253, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45686456863040253\n[LightGBM] [Warning] feature_fraction is set=0.968834296976213, colsample_bytree=0.8613120267643509 will be ignored. Current value: feature_fraction=0.968834296976213\n[LightGBM] [Warning] lambda_l1 is set=0.7911669785745563, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7911669785745563\n[LightGBM] [Warning] bagging_fraction is set=0.770397367765257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770397367765257\n[LightGBM] [Warning] lambda_l2 is set=0.45686456863040253, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45686456863040253\n| \u001b[0m 3       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.7704  \u001b[0m | \u001b[0m 0.8613  \u001b[0m | \u001b[0m 0.9688  \u001b[0m | \u001b[0m 0.7912  \u001b[0m | \u001b[0m 0.4569  \u001b[0m | \u001b[0m 0.03905 \u001b[0m | \u001b[0m 4.72    \u001b[0m | \u001b[0m 74.79   \u001b[0m | \u001b[0m 343.0   \u001b[0m |\n[LightGBM] [Warning] feature_fraction is set=0.4105738543966193, colsample_bytree=0.5166298385325709 will be ignored. Current value: feature_fraction=0.4105738543966193\n[LightGBM] [Warning] lambda_l1 is set=0.2619100559835807, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2619100559835807\n[LightGBM] [Warning] bagging_fraction is set=0.935227760987383, subsample=1.0 will be ignored. Current value: bagging_fraction=0.935227760987383\n[LightGBM] [Warning] lambda_l2 is set=0.7664913525398744, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7664913525398744\n[LightGBM] [Warning] feature_fraction is set=0.4105738543966193, colsample_bytree=0.5166298385325709 will be ignored. Current value: feature_fraction=0.4105738543966193\n[LightGBM] [Warning] lambda_l1 is set=0.2619100559835807, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2619100559835807\n[LightGBM] [Warning] bagging_fraction is set=0.935227760987383, subsample=1.0 will be ignored. Current value: bagging_fraction=0.935227760987383\n[LightGBM] [Warning] lambda_l2 is set=0.7664913525398744, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7664913525398744\n[LightGBM] [Warning] feature_fraction is set=0.4105738543966193, colsample_bytree=0.5166298385325709 will be ignored. Current value: feature_fraction=0.4105738543966193\n[LightGBM] [Warning] lambda_l1 is set=0.2619100559835807, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2619100559835807\n[LightGBM] [Warning] bagging_fraction is set=0.935227760987383, subsample=1.0 will be ignored. Current value: bagging_fraction=0.935227760987383\n[LightGBM] [Warning] lambda_l2 is set=0.7664913525398744, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7664913525398744\n[LightGBM] [Warning] feature_fraction is set=0.4105738543966193, colsample_bytree=0.5166298385325709 will be ignored. Current value: feature_fraction=0.4105738543966193\n[LightGBM] [Warning] lambda_l1 is set=0.2619100559835807, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2619100559835807\n[LightGBM] [Warning] bagging_fraction is set=0.935227760987383, subsample=1.0 will be ignored. Current value: bagging_fraction=0.935227760987383\n[LightGBM] [Warning] lambda_l2 is set=0.7664913525398744, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7664913525398744\n[LightGBM] [Warning] feature_fraction is set=0.4105738543966193, colsample_bytree=0.5166298385325709 will be ignored. Current value: feature_fraction=0.4105738543966193\n[LightGBM] [Warning] lambda_l1 is set=0.2619100559835807, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2619100559835807\n[LightGBM] [Warning] bagging_fraction is set=0.935227760987383, subsample=1.0 will be ignored. Current value: bagging_fraction=0.935227760987383\n[LightGBM] [Warning] lambda_l2 is set=0.7664913525398744, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7664913525398744\n| \u001b[0m 4       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.9352  \u001b[0m | \u001b[0m 0.5166  \u001b[0m | \u001b[0m 0.4106  \u001b[0m | \u001b[0m 0.2619  \u001b[0m | \u001b[0m 0.7665  \u001b[0m | \u001b[0m 0.02286 \u001b[0m | \u001b[0m 15.07   \u001b[0m | \u001b[0m 31.32   \u001b[0m | \u001b[0m 485.3   \u001b[0m |\n[LightGBM] [Warning] feature_fraction is set=0.9343162229216264, colsample_bytree=0.6107646569060093 will be ignored. Current value: feature_fraction=0.9343162229216264\n[LightGBM] [Warning] lambda_l1 is set=0.6750020961124485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6750020961124485\n[LightGBM] [Warning] bagging_fraction is set=0.6060135559229249, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6060135559229249\n[LightGBM] [Warning] lambda_l2 is set=0.35591282156804815, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.35591282156804815\n[LightGBM] [Warning] feature_fraction is set=0.9343162229216264, colsample_bytree=0.6107646569060093 will be ignored. Current value: feature_fraction=0.9343162229216264\n[LightGBM] [Warning] lambda_l1 is set=0.6750020961124485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6750020961124485\n[LightGBM] [Warning] bagging_fraction is set=0.6060135559229249, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6060135559229249\n[LightGBM] [Warning] lambda_l2 is set=0.35591282156804815, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.35591282156804815\n[LightGBM] [Warning] feature_fraction is set=0.9343162229216264, colsample_bytree=0.6107646569060093 will be ignored. Current value: feature_fraction=0.9343162229216264\n[LightGBM] [Warning] lambda_l1 is set=0.6750020961124485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6750020961124485\n[LightGBM] [Warning] bagging_fraction is set=0.6060135559229249, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6060135559229249\n[LightGBM] [Warning] lambda_l2 is set=0.35591282156804815, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.35591282156804815\n[LightGBM] [Warning] feature_fraction is set=0.9343162229216264, colsample_bytree=0.6107646569060093 will be ignored. Current value: feature_fraction=0.9343162229216264\n[LightGBM] [Warning] lambda_l1 is set=0.6750020961124485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6750020961124485\n[LightGBM] [Warning] bagging_fraction is set=0.6060135559229249, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6060135559229249\n[LightGBM] [Warning] lambda_l2 is set=0.35591282156804815, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.35591282156804815\n[LightGBM] [Warning] feature_fraction is set=0.9343162229216264, colsample_bytree=0.6107646569060093 will be ignored. Current value: feature_fraction=0.9343162229216264\n[LightGBM] [Warning] lambda_l1 is set=0.6750020961124485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6750020961124485\n[LightGBM] [Warning] bagging_fraction is set=0.6060135559229249, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6060135559229249\n[LightGBM] [Warning] lambda_l2 is set=0.35591282156804815, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.35591282156804815\n| \u001b[0m 5       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.606   \u001b[0m | \u001b[0m 0.6108  \u001b[0m | \u001b[0m 0.9343  \u001b[0m | \u001b[0m 0.675   \u001b[0m | \u001b[0m 0.3559  \u001b[0m | \u001b[0m 0.02191 \u001b[0m | \u001b[0m 18.05   \u001b[0m | \u001b[0m 34.22   \u001b[0m | \u001b[0m 500.0   \u001b[0m |\n"
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                 \u001b[0mx_probe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Queue is empty, no more objects to retrieve.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: Queue is empty, no more objects to retrieve.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-472d1822c56c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# acq='ei'사용\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# xi=0.01 로 exploration의 강도를 조금 높임\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlgbmBO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ei'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[0mx_probe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36msuggest\u001b[1;34m(self, utility_function)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# Finding argmax of the acquisition function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_vector_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             X, y = self._validate_data(X, y, multi_output=True, y_numeric=True,\n\u001b[0m\u001b[0;32m    193\u001b[0m                                        ensure_2d=True, dtype=\"numeric\")\n\u001b[0;32m    194\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    821\u001b[0m                     estimator=estimator)\n\u001b[0;32m    822\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[0m\u001b[0;32m    824\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[0;32m    825\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00027-39e2709d-7ecd-469d-9561-f44aede4c637",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 290.96875,
    "deepnote_output_heights": [
     194.03125
    ]
   },
   "source": "lgbmBO.max",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "{'target': nan,\n 'params': {'bagging_fraction': 0.5433704875376587,\n  'colsample_bytree': 0.7080374727086953,\n  'feature_fraction': 0.5967754659733202,\n  'lambda_l1': 0.5394343511669278,\n  'lambda_l2': 0.41941825134551564,\n  'learning_rate': 0.03233011624202614,\n  'max_depth': 12.064505859041928,\n  'min_child_weight': 92.42411005474558,\n  'num_leaves': 589.0988281503088}}"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00028-7d1c5a11-3a0c-4034-b5ad-32628b2babbe",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 245.953125
   },
   "source": "#파라미터 적용\nfit_lgbm = lgbm.LGBMClassifier(learning_rate=lgbmBO.max['params']['learning_rate'],\n                               num_leaves = int(round(lgbmBO.max['params']['num_leaves'])),\n                               max_depth = int(round(lgbmBO.max['params']['max_depth'])),\n                               min_child_weight = int(round(lgbmBO.max['params']['min_child_weight'])),\n                               colsample_bytree=lgbmBO.max['params']['colsample_bytree'],\n                               feature_fraction = max(min(lgbmBO.max['params']['feature_fraction'], 1), 0),\n                               bagging_fraction = max(min(lgbmBO.max['params']['bagging_fraction'], 1), 0),\n                               lambda_l1 = lgbmBO.max['params']['lambda_l1'],\n                               lambda_l2 = lgbmBO.max['params']['lambda_l2']\n                               )",
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00029-8c2833cd-86ab-44d8-b05a-239facbdff04",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 173.28125
   },
   "source": "model = fit_lgbm.fit(x_train_transformed, y_train)",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[LightGBM] [Warning] feature_fraction is set=0.5967754659733202, colsample_bytree=0.7080374727086953 will be ignored. Current value: feature_fraction=0.5967754659733202\n[LightGBM] [Warning] lambda_l1 is set=0.5394343511669278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5394343511669278\n[LightGBM] [Warning] bagging_fraction is set=0.5433704875376587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5433704875376587\n[LightGBM] [Warning] lambda_l2 is set=0.41941825134551564, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41941825134551564\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00030-fa3d6f5e-13b2-4ceb-88f3-61e645fbb228",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 65.953125
   },
   "source": "BO_predicted = model.predict(x_test_transformed)",
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00031-dde88afc-0e26-4ad6-9beb-904d9b142f76",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 442.34375
   },
   "source": "# clf.fit(X=x_train_transformed, y=y_train) \n# print(clf.best_params_) \n# predicted=clf.predict(x_test_transformed) \n# print('Classification of the result is:')\n# print(accuracy_score(y_test, BO_predicted))\n\nprint(\"정밀도(Precision Score) : \",precision_score(y_test, BO_predicted, pos_label='positive',average='micro'))\nprint(\"재현율(Recall Score) : \",recall_score(y_test, BO_predicted, \n                                       pos_label='positive'\n                                       ,average='micro'))\nprint ('F1-score (정밀도와 재현율의 조화평균): ', round(f1_score(y_test, BO_predicted, average='micro'), ndigits=4))\nprint ('혼동행렬: \\n', confusion_matrix(y_test, BO_predicted))\n# metrics(y_test, pred_test, \"lightGBM\")",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "정밀도(Precision Score) :  0.6914193635020813\n재현율(Recall Score) :  0.6914193635020813\nF1-score (정밀도와 재현율의 조화평균):  0.6914\n혼동행렬: \n [[   0  118  792]\n [   0  412 1368]\n [   0   20 4737]]\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00032-f1c0ca2a-596d-4f68-8aa9-4bc5747d8e54",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 479.03125,
    "deepnote_output_heights": [
     382.078125
    ]
   },
   "source": "y_test#(7447, 1)",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>credit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20269</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>21539</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>13968</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>14641</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>22954</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1952</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>650</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>9735</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3048</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>22591</th>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7447 rows × 1 columns</p>\n</div>",
      "text/plain": "       credit\n20269     2.0\n21539     2.0\n13968     2.0\n14641     1.0\n22954     2.0\n...       ...\n1952      2.0\n650       2.0\n9735      0.0\n3048      2.0\n22591     2.0\n\n[7447 rows x 1 columns]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00033-2b339cc9-a92b-4f2e-a154-a6242adb1a7d",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 118.125,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "BO_predicted#(7447,)",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2., 2., 2., ..., 2., 2., 2.])"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 결과 비교",
   "metadata": {
    "cell_id": "00034-281e87d6-0a8d-43bf-adeb-a16120a893c1",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 81.953125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00035-1681195b-14ba-4c98-98b7-eb9bbe21fa43",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 208.125,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "BOcm = confusion_matrix(y_test, BO_predicted)\n# cm.sum(axis=0) #[  86,  766, 6595]\n# cm.sum(axis=1) #[ 910, 1780, 4757]\n\nBOcm.sum(axis=0)#[   0,  550, 6897]\nBOcm.sum(axis=1) #[ 910, 1780, 4757]",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 910, 1780, 4757], dtype=int64)"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00036-18691619-2553-4c37-99da-244a8bb2e848",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1600.9375
   },
   "source": "# LightGBM 임포트\nfrom lightgbm import LGBMClassifier\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n\nparams: {'bagging_fraction': 0.5433704875376587, # 오버피팅 방지를 위해 데이터 일부만 사용\n  'colsample_bytree': 0.7080374727086953, #트리 생성에 필요한 샘플링에서 사용할 범위\n  'feature_fraction': 0.5967754659733202, #열 샘플링\n  'lambda_l1': 0.5394343511669278, # L1 정규화로 오버피팅 방지\n  'lambda_l2': 0.41941825134551564, # L2 정규화\n  'learning_rate': 0.03233011624202614, #학습률\n  'max_depth': 12.064505859041928, # 트리의 최대 깊이\n  'min_child_weight': 92.42411005474558,#leaf와 유사, 오버피팅 방지 너무 크면 오버피팅\n  'num_leaves': 589.0988281503088, # 트리가 가질 수 있는 최대 leaf 갯수, 너무 크면 정확도와 복잡도 증가\n  'objective' : 'multiclass', #다중 분류\n  'num_class': 3, # 클래스 3개 (0, 1, 2)\n  'metric' : 'multi_logloss',\n'boosting_type' : 'gbdt'} \n    \n\n# LGBM 분류기 객체 생성\nlgbm_wrapper = LGBMClassifier(**params)\n\n# 학습, 테스트 데이터 분리\n# X_train, X_test, y_train, y_test = train_test_split(ftr, target, test_size=0.2)\n\n# model.fit(x_train_transformed, y_train)\n\n\n# 조기 중단 기능에 필요한 파라미터 정의\nevals = [(x_test_transformed, y_test)]\nlgbm_wrapper.fit(x_train_transformed, y_train, early_stopping_rounds=100, eval_metric='multi_logloss', eval_set=evals, verbose=True)\n\npreds = lgbm_wrapper.predict(x_test_transformed)\npred_proba = lgbm_wrapper.predict_proba(x_test_transformed)[:,1]\n\n\n# 다양한 오차 측정 지표를 확인하기 위한 함수 정의\n\nfrom sklearn.metrics import *\n\nprint(\"Precision Score 정밀도 : \",precision_score(y_test, BO_predicted, pos_label='positive',average='macro'))\nprint(\"Recall Score 재현율 : \",recall_score(y_test, BO_predicted, \n                                           pos_label='positive'\n                                           ,average='macro')) \nprint ('F1 정밀도와 재현율의 조화평균: ', round(f1_score(y_test, BO_predicted, average='macro'), ndigits=4))\nprint ('Confusion 혼동행렬: \\n', confusion_matrix(y_test, BO_predicted))",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1]\tvalid_0's multi_logloss: 0.866845\n[2]\tvalid_0's multi_logloss: 0.853139\n[3]\tvalid_0's multi_logloss: 0.843271\n[4]\tvalid_0's multi_logloss: 0.835305\n[5]\tvalid_0's multi_logloss: 0.828628\n[6]\tvalid_0's multi_logloss: 0.823379\n[7]\tvalid_0's multi_logloss: 0.818241\n[8]\tvalid_0's multi_logloss: 0.813998\n[9]\tvalid_0's multi_logloss: 0.810795\n[10]\tvalid_0's multi_logloss: 0.807598\n[11]\tvalid_0's multi_logloss: 0.804262\n[12]\tvalid_0's multi_logloss: 0.801983\n[13]\tvalid_0's multi_logloss: 0.800288\n[14]\tvalid_0's multi_logloss: 0.798227\n[15]\tvalid_0's multi_logloss: 0.796948\n[16]\tvalid_0's multi_logloss: 0.795744\n[17]\tvalid_0's multi_logloss: 0.794448\n[18]\tvalid_0's multi_logloss: 0.792959\n[19]\tvalid_0's multi_logloss: 0.791874\n[20]\tvalid_0's multi_logloss: 0.791299\n[21]\tvalid_0's multi_logloss: 0.790098\n[22]\tvalid_0's multi_logloss: 0.78952\n[23]\tvalid_0's multi_logloss: 0.788503\n[24]\tvalid_0's multi_logloss: 0.787894\n[25]\tvalid_0's multi_logloss: 0.787137\n[26]\tvalid_0's multi_logloss: 0.786117\n[27]\tvalid_0's multi_logloss: 0.785349\n[28]\tvalid_0's multi_logloss: 0.784823\n[29]\tvalid_0's multi_logloss: 0.78394\n[30]\tvalid_0's multi_logloss: 0.783437\n[31]\tvalid_0's multi_logloss: 0.782892\n[32]\tvalid_0's multi_logloss: 0.782653\n[33]\tvalid_0's multi_logloss: 0.782269\n[34]\tvalid_0's multi_logloss: 0.781788\n[35]\tvalid_0's multi_logloss: 0.781528\n[36]\tvalid_0's multi_logloss: 0.781285\n[37]\tvalid_0's multi_logloss: 0.781073\n[38]\tvalid_0's multi_logloss: 0.780417\n[39]\tvalid_0's multi_logloss: 0.780042\n[40]\tvalid_0's multi_logloss: 0.779407\n[41]\tvalid_0's multi_logloss: 0.779128\n[42]\tvalid_0's multi_logloss: 0.778762\n[43]\tvalid_0's multi_logloss: 0.7788\n[44]\tvalid_0's multi_logloss: 0.77872\n[45]\tvalid_0's multi_logloss: 0.778564\n[46]\tvalid_0's multi_logloss: 0.778168\n[47]\tvalid_0's multi_logloss: 0.778033\n[48]\tvalid_0's multi_logloss: 0.777449\n[49]\tvalid_0's multi_logloss: 0.777135\n[50]\tvalid_0's multi_logloss: 0.77711\n[51]\tvalid_0's multi_logloss: 0.776941\n[52]\tvalid_0's multi_logloss: 0.776616\n[53]\tvalid_0's multi_logloss: 0.77593\n[54]\tvalid_0's multi_logloss: 0.775477\n[55]\tvalid_0's multi_logloss: 0.774975\n[56]\tvalid_0's multi_logloss: 0.77446\n[57]\tvalid_0's multi_logloss: 0.774317\n[58]\tvalid_0's multi_logloss: 0.774014\n[59]\tvalid_0's multi_logloss: 0.77368\n[60]\tvalid_0's multi_logloss: 0.773111\n[61]\tvalid_0's multi_logloss: 0.773133\n[62]\tvalid_0's multi_logloss: 0.772925\n[63]\tvalid_0's multi_logloss: 0.772778\n[64]\tvalid_0's multi_logloss: 0.772629\n[65]\tvalid_0's multi_logloss: 0.772642\n[66]\tvalid_0's multi_logloss: 0.772809\n[67]\tvalid_0's multi_logloss: 0.772638\n[68]\tvalid_0's multi_logloss: 0.772646\n[69]\tvalid_0's multi_logloss: 0.772468\n[70]\tvalid_0's multi_logloss: 0.772243\n[71]\tvalid_0's multi_logloss: 0.772082\n[72]\tvalid_0's multi_logloss: 0.77208\n[73]\tvalid_0's multi_logloss: 0.771975\n[74]\tvalid_0's multi_logloss: 0.771935\n[75]\tvalid_0's multi_logloss: 0.771844\n[76]\tvalid_0's multi_logloss: 0.771916\n[77]\tvalid_0's multi_logloss: 0.771578\n[78]\tvalid_0's multi_logloss: 0.771399\n[79]\tvalid_0's multi_logloss: 0.771455\n[80]\tvalid_0's multi_logloss: 0.771056\n[81]\tvalid_0's multi_logloss: 0.770821\n[82]\tvalid_0's multi_logloss: 0.770746\n[83]\tvalid_0's multi_logloss: 0.770668\n[84]\tvalid_0's multi_logloss: 0.770622\n[85]\tvalid_0's multi_logloss: 0.770766\n[86]\tvalid_0's multi_logloss: 0.770648\n[87]\tvalid_0's multi_logloss: 0.770809\n[88]\tvalid_0's multi_logloss: 0.770966\n[89]\tvalid_0's multi_logloss: 0.770888\n[90]\tvalid_0's multi_logloss: 0.770903\n[91]\tvalid_0's multi_logloss: 0.770736\n[92]\tvalid_0's multi_logloss: 0.770549\n[93]\tvalid_0's multi_logloss: 0.770597\n[94]\tvalid_0's multi_logloss: 0.770562\n[95]\tvalid_0's multi_logloss: 0.770755\n[96]\tvalid_0's multi_logloss: 0.770902\n[97]\tvalid_0's multi_logloss: 0.770572\n[98]\tvalid_0's multi_logloss: 0.770666\n[99]\tvalid_0's multi_logloss: 0.770578\n[100]\tvalid_0's multi_logloss: 0.770611\nPrecision Score 정밀도 :  0.4786370885892417\nRecall Score 재현율 :  0.40908544789897644\nF1 정밀도와 재현율의 조화평균:  0.3889\nConfusion 혼동행렬: \n [[   0  118  792]\n [   0  412 1368]\n [   0   20 4737]]\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00037-011755c0-6317-4349-a04d-2815b899cff6",
    "owner_user_id": "d308ea81-b419-4bb8-8121-ba7ad904b283",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 65.953125
   },
   "source": "print(accuracy_score(y_test, BO_predicted)) #0.6914193635020813",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6988cc49-778f-4dcf-a2dc-70fd016d5812' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "deepnote_notebook_id": "6b13b8e6-ee47-45c8-83b0-c8ff1fff5204",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}